#coding:utf-8
import urllib
import urllib2
import re
import MySQLdb
import webbrowser
import time
import random
from user_agents import agents

url_base = 'http://weibo.cn/'

cookie = '_T_WM=06aa2742862d051d51badecc90ec1e97; ' \
         'SUB=_2A256LpKsDeRxGeVM7FQT9ynJyT6IHXVZ0D7krDV6PUJbstBeLW_ekW1LHet-N3Ni6yCNagiIOADtqwglohgHLw..; ' \
         'SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WhmuP4Ef-Gwl4bzviKJL7hk5JpX5o2p5NHD95Q0eoMceoMNSKzE; ' \
         'SUHB=079jUm9K0x5Gji; SSOLoginState=1462428413; gsid_CTandWM=4uHmCpOz5g06MYxZaLYeUdKjb8q'

headers = {
    'User-Agent': random.choice(agents),
    'cookie':cookie
}

#负责将微博内容抓到本地形成一个文本文件
def _spyder (nickname = 'kaifulee'):
    filename = "f://%s.txt" % nickname
    #打开本地的存储玩野源代码的文件 读取并定位当前抓取到的位置
    try:
        __file = open(filename, 'r')
        lis = __file.read()
        re_one = re.compile('<number>\d+</number>')
        num = re_one.findall(lis)
        re_num = re.compile('\d+')
        start = 0
        for every in num:
            temp = int(re_num.findall(every)[0])
            print 'temp:' + str(temp)
            if (start < temp):
                start = temp
            else:
                pass
        print int(start)
        __file.close()
    except Exception, e:
        start = 0
    finally:
        pass
    #

    _file = open(filename, 'w+')
    try:
        #open file
        #start web spider
        for count in range(start + 1, 9999):
            time.sleep(int(random.random() * 5))
            url = "http://weibo.cn/%s?page=%d" % (nickname, count)
            req = urllib2.Request(url, headers=headers)
            req.add_header('Accept','text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')
            req.add_header('Connection','keep-alive')
            text = urllib2.urlopen(req).read()
            #regular match
            pattern = re.compile(r'<div class="c"(.+?)<div class="s"></div>')
            string = pattern.findall(text)
            #write in file
            for item in string:
                _file.write(item + '\n')
                print item
            _file.write('<number>' + str(count) + '</number>' + '\n')
            print '<number>' + str(count) + '</number>' + '\n'
    except Exception, e:
        print Exception,":",e
        _file.close()
    finally:
        pass
    _file.close()


#查询函数 通过get方法访问weibo.cn/search 将所要查询的内容提交给后台 返回一个查询结果
#但是目前存在问题，搜索只能获取到第一页的结果，后续页的访问会遇到403错误，留待解决
def _search(name):
    url = url_base + "/search/?pos=search"
    req = urllib2.Request(url, headers=headers)
    req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')
    req.add_header('Connection', 'keep-alive')
    data = 'keyword='+ urllib.quote(name) +'&suser=%E6%89%BE%E4%BA%BA'
    req.add_data(data)
    #上面是构造初次访问的访问包

    text_final = ''  # 将搜索页面的全部结果汇总到一个对象中 便于处理
    text_final += urllib2.urlopen(req).read()

    #获取到总的搜索页数 之后通过页码进行后续搜索页的访问
    '''
    _cmp_page = re.compile(r'<input name="mp" type="hidden" value="(\d)"')
    num_page = int(_cmp_page.findall(text_final)[0])
    for count in range(2,num_page+1):
        url_next = 'http://weibo.cn/search/mblog?keyword=%s&page=%d' % (urllib.quote(name),count)
        text_final += urllib2.urlopen(req).read()
    '''
    #这部分代码由于经常出现403Forbidden错误而难以使用，在排出问题之前只能将其注释掉了
    #这意味着只能获取到搜索页的第一页内容

    _cmp_block = re.compile(r'<table>(.+?)</table>')
    rst_block = _cmp_block.findall(text_final)
    #定义一个list用于返回整个搜索的结果（目前只能返回10条）
    lst_rst = []
    for item in rst_block:
        lst_rst.append(_find_rst(item))
    return lst_rst




def _find_rst(txt):
    dic = {}
    #构造匹配搜索结果url的模式
    try:
        _cmp_url = re.compile(r'<a href="(.+?)"')
        rst_match = _cmp_url.findall(txt)[0]
        dic['url'] = rst_match[0:len(rst_match) - 11]
        _cmp_fans = re.compile(r'粉丝(\d+)')
        dic['fans'] = int(_cmp_fans.findall(txt)[0])
        _cmp_loc = re.compile(r'&nbsp;(.+?)<br/>')
        dic['loc'] = _cmp_loc.findall(txt)[0]
        # 构造匹配姓名的模式
        _cmp_name = re.compile(r'<a href="%s.+?\D">(.+?)</a>' % dic['url'])
        name_tmp = _cmp_name.findall(txt)[0]
        dic['name'] = re.findall(r'<a href="%s.+">(.+?)$' % dic['url'],name_tmp)[0]
        return dic
    except Exception , e:
        print "Error occur:%s"% e
        return -1



def _search_rst_insert(dic):
    print dic
    time_now = time.strftime('%Y-%m-%d',time.localtime(time.time()))
    connection = MySQLdb.connect(host = 'localhost',user = 'root',passwd = 'root',db = 'test',charset='utf8')
    cursor = connection.cursor()
    try:
        cursor.execute('create table rst_search (id int primary key auto_increment,name text,url text,location text,fans int,cnt_date datetime)')
        connection.commit()
    except Exception,e:
        pass
    #设置自增序号作为id
    try:
        for item in dic:
            query_insert = 'insert into rst_search(name,url,location,fans,cnt_date) values("%s","%s","%s",%d,"%s")'\
                           %(item['name'],item['url'],item['loc'],item['fans'],time_now)
            cursor.execute(query_insert)
            connection.commit()
    except Exception,e:
        print 'Error occur:%s' % e
        connection.close()
        return -1
    print 'Insert successful!'
    connection.close()
    return 0

if __name__ == '__main__':
    _spyder()
  #  search()

